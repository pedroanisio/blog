# Mathematical Approaches for Assessing Code Similarity

**Introduction**  
Assessing code similarity is crucial in software engineering for detecting duplicates (clones), plagiarism, and ensuring consistent maintenance. There are three broad categories of techniques for measuring how similar two pieces of code are: **lexical** (text-based) methods, **structural** (syntax-based) methods, and **semantic** (meaning-based) methods. Each approach uses different mathematical foundations – from simple term frequency counts to tree edit distances and high-dimensional neural embeddings. This report explores each category in depth, comparing their strengths, weaknesses, and use cases. We also discuss how hybrid techniques combine multiple approaches and outline key applications (like plagiarism detection, refactoring, and debugging) and future directions.

## Lexical Similarity Methods

Lexical methods treat code as text or sequences of tokens. They typically ignore the deeper structure or meaning of the code, focusing instead on surface-level features like the frequency of words (identifiers, keywords) or token sequences.

### TF-IDF Vectorization of Code  
One common lexical approach is to represent code using **term frequency–inverse document frequency (TF-IDF)** vectors. In this approach, each code snippet (or file/function) is treated as a “document” and each token (word) as a term. A TF-IDF vector for a code snippet encodes how often each token appears (term frequency) weighted by how unique that token is across a corpus of code (inverse document frequency). This yields a high-dimensional vector where each dimension corresponds to a token and its value reflects that token’s importance in the snippet. Rare tokens (e.g. a specific API call or an uncommon variable name) get higher weight than common tokens (e.g. `{`, `}`, `for`, `if`). By vectorizing code in this way, we can quantitatively compare code snippets using numerical vector operations.

For example, consider two functions that both implement a bubble sort algorithm in slightly different ways. A TF-IDF representation will capture that both functions share many uncommon tokens (like specific variable names or API calls related to swapping) and thus produce somewhat similar weighted term vectors. In contrast, entirely different code (say, a sorting algorithm versus a database connection function) will have very different tokens and thus divergent TF-IDF vectors. TF-IDF is straightforward to compute and is language-independent – it can be applied to any source code as long as you can tokenize the code into words. It is also scalable to large codebases because it converts text into numeric vectors that are efficient to compare ([An Introduction to TF-IDF: What It Is & How to Use It](https://www.semrush.com/blog/tf-idf/#:~:text=,collection%20of%20documents%E2%80%94it%20helps%20to)) ([An Introduction to TF-IDF: What It Is & How to Use It](https://www.semrush.com/blog/tf-idf/#:~:text=%2A%20Language,a%20large%20number%20of%20documents)).

However, TF-IDF on code has the same fundamental limitation it has on natural language: it **does not understand meaning or order** ([An Introduction to TF-IDF: What It Is & How to Use It](https://www.semrush.com/blog/tf-idf/#:~:text=,have%20difficulties%20recognizing%20synonyms%20and)). It treats code as a “bag of words.” Thus, two code snippets with different structure or logic can appear dissimilar in TF-IDF if they don’t share tokens, even if they accomplish the same task. Conversely, code that shares many tokens (perhaps due to boilerplate or common library calls) can have high TF-IDF similarity despite doing unrelated things. We will discuss these strengths and weaknesses more below.

### Cosine Similarity for Code Comparison  
Once code snippets are represented as TF-IDF vectors (or other lexical vectors), **cosine similarity** is a common metric to quantify their similarity. Cosine similarity measures the cosine of the angle between two vectors in the high-dimensional term space. Given two code vectors **A** and **B**, the cosine similarity is `cos(theta) = (A · B) / (||A|| * ||B||)`, producing a score from 0 (orthogonal, meaning completely different tokens) to 1 (identical direction, meaning the code shares the exact same token frequency profile). In practice, after TF-IDF vectorization each code snippet has a vector of weights for tokens, and cosine similarity allows us to see if two snippets emphasize the same tokens to a similar degree.

For example, using TF-IDF + cosine, a simple plagiarism detector for code would compute the cosine similarity between a student's program and a repository of existing programs. If a student copied large portions of code, the vectors will be very close and yield a cosine similarity near 1.0, indicating a high degree of lexical overlap. This approach has been implemented in various plagiarism detection tools and academic studies, sometimes combined with thresholding (e.g., flagging pairs with cosine similarity above a certain cutoff). 

**Strengths and Weaknesses of Lexical Approaches:**  

- **Strengths:** Lexical techniques are relatively simple and fast to compute. They require no complex parsing – treating code as plain text – which makes them language-agnostic in many cases (aside from basic tokenization) ([An Introduction to TF-IDF: What It Is & How to Use It](https://www.semrush.com/blog/tf-idf/#:~:text=,collection%20of%20documents%E2%80%94it%20helps%20to)). They excel at catching Type-1 and some Type-2 code clones (exact copies or copies with consistent renaming) because such clones share a lot of tokens. Tools based on token counting or hashing (e.g., running Karp-Rabin string matching or winnowing algorithms) have been effective for detecting straightforward duplicates and plagiarism where the code hasn’t been heavily modified. Lexical similarity can also be scaled to large corpora; for instance, search engines can index code by tokens and quickly retrieve candidates via inverted indices and then rank by cosine similarity.

- **Weaknesses:** Purely lexical methods ignore the **structure and semantics** of code. They do not understand the order of tokens or the program’s logic ([An Introduction to TF-IDF: What It Is & How to Use It](https://www.semrush.com/blog/tf-idf/#:~:text=,have%20difficulties%20recognizing%20synonyms%20and)). Thus, they can be easily fooled by simple transformations: reordering code blocks, changing variable names, or refactoring code (like unrolling a loop into repeated statements) can drastically reduce lexical similarity even if the functionality remains identical. For example, one snippet printing a message 5 times in a loop versus another snippet printing the same message with 5 separate `print` statements have very different token sequences. A lexical comparison might judge them dissimilar because one contains a `for` loop token while the other has multiple `print` tokens. In reality, these are functionally equivalent (a Type-3 or Type-4 clone), but a bag-of-words model likely fails to catch that. Conversely, lexical methods can produce false positives when code is superficially similar. A classic case: two programs that have a similar scaffold or use similar libraries, but actually solve different problems, might share many tokens (common language keywords, API calls) and appear similar in TF-IDF/cosine. For instance, a `TemperatureConverter` class and a `CurrencyConverter` class have similar structure and even share words like `convert`, but they implement completely different conversions (degrees vs. currency). Many lexical measures (and other unsophisticated similarity measures) would erroneously flag them as similar (a false clone) because of the overlapping words and structure. Overall, lexical approaches *“have no understanding of the meaning behind the terms or the context in which they’re used”* ([An Introduction to TF-IDF: What It Is & How to Use It](https://www.semrush.com/blog/tf-idf/#:~:text=,have%20difficulties%20recognizing%20synonyms%20and)), so they cannot reliably detect semantic equivalence or distinction if it’s not reflected in shared vocabulary.

## Structural Similarity Methods

Structural methods go beyond raw text to analyze the syntactic structure of code. The idea is to compare code based on its parse structure or program schema, rather than exact token sequences. These approaches often use compiler-style analysis: parsing code into abstract syntax trees or other intermediate representations and then measuring similarity on those structures.

### Abstract Syntax Trees (AST)  
An **Abstract Syntax Tree (AST)** represents the hierarchical structure of source code. Nodes in the tree correspond to language constructs (e.g., expressions, statements, blocks), and the tree captures the nesting and order inherent in the code’s grammar. To compare two code fragments structurally, one can parse both into ASTs and then evaluate how similar these trees are. For example, two functions that both have an `if-else` inside a loop will have ASTs with a similar shape (a loop node containing an if-statement node, etc.), even if variable names or literal values differ.

AST-based similarity can catch clones that have been lexically modified but maintain the same structure (so-called Type-2 clones with renamed identifiers, or even Type-3 clones with added/deleted statements). Consider two code fragments that compute a sum: one uses `for(int i=0; i<n; i++){ total += arr[i]; }` and another uses `for(int j=0; j<m; j++){ sum += array[j]; }`. Lexically, many tokens differ (`i` vs `j`, `n` vs `m`, `total` vs `sum`, `arr` vs `array`), but structurally, both are a for-loop with similar body. An AST comparison would recognize that both have the pattern “for-loop iterating n times, accumulating into a variable” with only leaf identifiers different. In fact, AST-based clone detectors typically *normalize* or ignore the actual identifier names, focusing on the tree shape and node types. By renaming all user-defined identifiers to a generic placeholder (e.g., all variable names to `v`), we can make the ASTs align even when the original variables differ. This **identifier renaming normalization** is a standard technique to handle Type-2 clones (which differ only in names or formatting).

Beyond just tree shape, structural approaches might also consider specific *structures* in code like control flow. For instance, some methods incorporate **control-flow graphs (CFGs)** or program dependence graphs, which capture the flow of execution and dependencies. These can detect similarity in control logic even if the ASTs differ in some details. A study by Jiang et al. (Deckard tool) demonstrated creating **characteristic vectors** from AST subtrees to cluster and identify code clones at scale, leveraging tree structure while using vectorization for efficiency.

### Tree Edit Distance for Code Comparison  
A rigorous way to quantify AST similarity is via **tree edit distance**. Tree edit distance is an extension of the string edit distance (Levenshtein distance) concept to tree-structured data. It is defined as the minimum cost sequence of edit operations (inserting, deleting, or changing nodes) required to transform one tree into another. If two code fragments are identical, the AST edit distance is zero (no edits needed). Small distances indicate minor changes in structure (perhaps an extra statement or a different literal value), whereas large distances indicate significant structural divergence.

Mathematically, computing the exact tree edit distance can be expensive (the general problem is NP-hard), but optimized algorithms (like APTED by Pawlik and Augsten) can handle ordered tree edit distance reasonably for moderate-sized trees. In practice, AST edit distance has been used as a similarity metric: two code snippets are considered similar if their AST edit distance is below a threshold (or inversely, a normalized similarity score is above a threshold). Recent research highlights that AST edit distance is effective in capturing *“intricate code structures”* that might be missed by purely token-based metrics. In a 2024 study, Song *et al.* found that AST edit-based similarity had a high correlation with more intuitive code similarity measures (like functional correctness and manual evaluations), outperforming n-gram overlap (BLEU score) and set overlap (Jaccard) in recognizing structurally similar code.

**Identifier Renaming and Normalization:** A key step in structural comparison is normalizing the ASTs to eliminate irrelevant differences. This includes: 
- **Renaming identifiers consistently:** All variable names, function names, and sometimes literal values can be replaced with generic placeholders so that two code fragments that only differ in naming are treated as the same. This is crucial for detecting Type-2 clones. For example, renaming all variables to `var1, var2, ...` in both ASTs will cause a function using `x = a + b;` and another using `total = num1 + num2;` to look identical structurally. 
- **Normalization of syntax:** This can involve pretty-printing code (standardizing formatting), removing comments and whitespace, and even sorting or canonicalizing certain syntactic elements if order doesn’t impact semantics. Tools like NiCad (Roy and Cordy, 2008) use pretty-printing and consistent formatting as a form of source code normalization to improve clone detection.
- **AST pruning or abstraction:** Some approaches abstract away certain substructures or data types. For instance, literal values might be replaced by a generic `<NUM>` token, or specific subtrees (like blocks of arithmetic operations) might be considered as single nodes if we want to be tolerant of their internal differences.

Normalization can significantly improve the recall of structural clone detection. One empirical study found that using compilation and decompilation as a normalization (essentially, parsing code and then regenerating source from bytecode) eliminated all false mismatches in several clone detection tools. By normalizing, the code fragments that are semantically equivalent tend to converge to a similar representation, allowing structural metrics to correctly identify them as clones.

### Performance Metrics and Limitations of Structural Methods  
Structural similarity methods are typically evaluated on their *accuracy* in clone detection (recall and precision for known clone pairs) and their *efficiency* on large codebases. They have some clear advantages over lexical methods: by considering structure, they can detect clones that lexical methods miss. For example, an AST-based technique would correctly realize that the loop-unrolled vs loop version of printing “Welcome to Java” are actually the same pattern (just one has a loop node with a print inside, versus five print statement nodes in sequence) – something a lexical approach struggled with. AST comparison is also less sensitive to identifier renaming thanks to normalization. Thus, structural approaches handle many Type-2 clones and even some Type-3 (where lines are added/deleted but overall structure remains similar).

However, structural methods have their *own* weaknesses and costs:
- **Semantic Blind Spots:** Structural similarity still doesn’t guarantee semantic equivalence. It’s possible for code to have an almost identical AST and yet do different things. The earlier example of `TemperatureConverter` vs `CurrencyConverter` illustrates this – both classes might have the same AST shape (a class with one method returning a computed value) and thus a high AST similarity (indeed, an AST comparison might score them as 81% similar in one example ([Source Code Clone Detection Using Unsupervised Similarity Measures](https://arxiv.org/html/2401.09885v1#:~:text=Table%201%20compares%20various%20unsupervised,would%20be%20good))). But their functionality is unrelated (one converts temperature units, the other currency). Structural methods can produce false positives by labeling such cases as similar. In contrast, two code fragments with very different ASTs might still be semantically similar (e.g., one uses iteration, another uses recursion to achieve the same result), and a pure AST-edit distance might score them as dissimilar. In summary, structural approaches capture the “form” of the code but not the full intent.
- **Complexity and Scalability:** Comparing tree structures is more computationally intensive than comparing text. A naive tree edit distance between two ASTs with *n* nodes can be quite slow (cubic or worse in time complexity). In large software projects with millions of lines of code, a pairwise AST comparison between every pair of functions is impractical. Therefore, practical clone detectors using ASTs employ heuristics or indexing to narrow down candidate pairs (e.g., hashing subtrees, comparing only certain significant substructures, or clustering via feature vectors as Deckard does). Despite these optimizations, AST-based analysis tends to be heavier than token-based scanning.
- **Normalization trade-offs:** While normalization improves recall, it can sometimes harm precision. Over-normalizing code (e.g., renaming all identifiers blindly) might cause distinct code to appear similar. There is a balance between removing cosmetic differences and retaining meaningful ones. For instance, renaming all identifiers will miss the fact that two pieces of code with different variable names might actually be using those variables in incompatible ways (if one expects a variable to be, say, a file handle and the other a loop index, the rest of the code usage might differ despite the same control flow structure).
- **Evaluation metrics:** Researchers often evaluate structural similarity methods on benchmark datasets like BigCloneBench or other clone repositories by measuring recall (how many known clone pairs are found) and precision (how many reported clones are true clones). AST-based methods historically perform well on Type-1 and Type-2 clones, and reasonably on Type-3. They struggle with Type-4 (semantic) clones unless the semantics happen to produce some structural similarity by coincidence or the method is augmented with additional analysis.

In recent evaluations, AST edit distance has shown a strong correlation with semantic correctness in tasks like code generation (e.g., text-to-SQL evaluations). This indicates that structural metrics can serve as a proxy for semantic similarity in many cases, arguably because a correct solution must use the right programming constructs. Nonetheless, purely structural metrics can be complemented by semantic-aware methods for the best results, as we discuss next.

## Semantic Similarity Methods

Semantic similarity approaches aim to measure how similar two code fragments are in terms of what they *mean* or what they *do*, rather than just how they look. These methods attempt to capture the functional equivalence of code. Two pieces of code are semantically similar if they produce the same outputs for the same inputs (or more loosely, if they implement the same specification), even if their text and structure differ significantly. This is a challenging problem – often as hard as solving program equivalence, which is generally undecidable in the general case. However, practical methods use heuristics, machine learning, and embeddings to approximate semantic understanding.

### Transformer-Based Models (CodeBERT, GraphCodeBERT, Codex, etc.)  
In recent years, **transformer-based models** pre-trained on large code corpora have achieved state-of-the-art results in capturing code semantics. Models like *CodeBERT* (2019) and *GraphCodeBERT* (2020) are extensions of the BERT architecture tailored for programming languages. They are trained on massive datasets of code (and sometimes paired with natural language docstrings) to learn a contextual representation of code tokens. The result is that given a code snippet, these models can produce a dense **embedding** (vector representation) that encapsulates not just the lexical information but also some understanding of syntax and even the intent of the code.

- **CodeBERT:** A bi-directional transformer trained on source code in multiple languages (and text), which learns to predict masked tokens or the association between code and its documentation. After training, the internal embeddings from CodeBERT can be used for similarity: if two code fragments have similar meaning, CodeBERT is likely to embed them close together in vector space. For clone detection tasks (like the BigCloneBench benchmark), CodeBERT achieved high scores, significantly outperforming traditional methods. For instance, on BigCloneBench (a dataset of known Java clone pairs including some semantic clones), CodeBERT-based models reach around 96% F1 score, detecting even non-trivial clones. This is far higher than simpler lexical or AST approaches on the same dataset, demonstrating the power of learned code semantics. However, it’s worth noting that BigCloneBench mostly contains easier clones (many with significant overlap); when researchers specifically test CodeBERT on truly deep semantic clones (very different code with same functionality), the accuracy drops – one study reported around 54–56% accuracy in detecting such *purely semantic* clones in Java and Python. This suggests that while CodeBERT captures some semantics, it still has limitations and may rely partly on stylistic or structural cues present in the training data.

- **GraphCodeBERT:** This model extends CodeBERT by incorporating structure explicitly. It uses not only the code tokens but also leverages the code’s data flow graph (DFG) as additional input to the transformer. By training on code with this graph structure, GraphCodeBERT is encouraged to learn relationships like data dependencies between variables, which are indicative of program semantics. GraphCodeBERT has shown improvements on tasks like code search and clone detection, as it can recognize semantic similarity even if code is reordered or uses different variable names, by focusing on how data flows and how the code is structured logically. An example from its authors: given two semantically equivalent code fragments, GraphCodeBERT’s output embeddings yield a high similarity score (~0.98 in one case) even if the code looks superficially different. The model effectively learns *“the intent and functionality behind different code fragments, even when those fragments appear different on the surface”*. This addresses a shortcoming of older methods which *“focus on surface-level similarity”* and miss deeper connections.

- **OpenAI Codex / GPT models:** Codex (based on GPT-3) is a large language model trained on a vast quantity of code from GitHub. While Codex is primarily known for code generation (e.g., powering GitHub Copilot), it also produces embeddings as part of its transformer architecture. OpenAI has released code and text embedding models (like `text-embedding-ada-002`) that can be used to get vector representations of code. These embeddings are designed such that semantically similar texts or code snippets map to nearby points in vector space. For example, given a snippet that sorts a list using bubble sort and another that uses insertion sort, a purely lexical approach would find them quite different (different sequences of operations), and even ASTs are different. But a well-trained code model might recognize both are sorting algorithms (perhaps via the context of comparisons and swaps) and yield embeddings that are closer than, say, an embedding of a completely unrelated program. Transformer models essentially try to *learn the semantics from context*: they see so many examples of code that they can infer, to some degree, the task being performed by a piece of code. In practical similarity tasks, one can take the embedding of one code piece (say from the final layer of CodeBERT or an embedding model) and compare it to the embedding of another code piece using cosine similarity. A high cosine similarity indicates the model believes these two perform similar functions.

One interesting application of large language models is prompting them to directly judge similarity. Researchers have experimented with using GPT-3.5/4 by providing two code snippets and asking the model (in natural language) to rate how similar they are. This uses the model’s internal knowledge and reasoning rather than a fixed embedding. Early results show GPT-4 can sometimes provide very insightful similarity judgments that correlate well with human assessments. However, these can be inconsistent – the same pair of code given in two different prompts might yield slightly different scores due to the nondeterministic nature of the model, and the approach is a black box (we don’t know *why* it judged them similar or different). Thus, while promising, prompt-based semantic scoring with GPT models requires careful handling to be reliable ([[2404.08817] Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance](https://ar5iv.org/html/2404.08817v2#:~:text=RQ2%3A%20How%20are%20TSED%20and,influence%20TSED)).

### How Embeddings Capture Functional Equivalence  
Embeddings from models like CodeBERT or Codex are vectors in a high-dimensional space (e.g., 768 dimensions for CodeBERT). The training objective of these models usually involves predicting masked tokens or next tokens, which forces the model to learn representations that encode the context. In code, context means the surrounding code elements, data types, and operations. Over many training examples, the model learns, for instance, that the presence of certain tokens like `sort`, `swap`, loops, and comparisons often go together – forming a concept of “sorting routine.” Similarly, a function that calculates Fibonacci might be recognized by the pattern of recursive calls or loop with specific arithmetic. These patterns of semantics get embedded in the vector.

Thus, two code snippets that implement the same underlying algorithm or logic will tend to activate similar internal neurons in the model, even if their actual text differs. The result: their embeddings have a small cosine distance (high similarity). In effect, the model has learned an abstract notion of the code’s *meaning*. This is why **embeddings can encode functional equivalence** to a certain extent. Empirical evidence of this is seen in the high performance of such models on clone detection benchmarks. For example, a clone detection task might present the model with a known clone pair of syntactically different implementations and ask, “are these similar?” Models like CodeBERT and GraphCodeBERT achieve high F1 scores, meaning they correctly cluster these as similar in embedding space. Another evidence: in the comparison of measures shown earlier (Table of various measures), an entry for “Code Embeddings Similarity” gave a near-perfect score (0.99) for known clones and also (perhaps too high) a 1.00 for a non-clone example ([Source Code Clone Detection Using Unsupervised Similarity Measures](https://arxiv.org/html/2401.09885v1#:~:text=Measure%20Score,35)) ([Source Code Clone Detection Using Unsupervised Similarity Measures](https://arxiv.org/html/2401.09885v1#:~:text=Metrics%20Similarity%200.98%201.00%20N,IDF%20Similarity%200.67%200.48)), suggesting the embedding used saw even the structurally similar non-clone as highly similar (which can be a caution: the embedding might have been over-generalizing in that instance).

One limitation is that these learned embeddings are only as good as the training data and objectives. They might latch onto spurious correlations (e.g., if in training data, many sorting algorithms use variable names like `i, j` and many other algorithms do not, the model might partly rely on that and get confused by a sorting algorithm that uses unusual variable names). They also might not understand aspects that require executing code mentally (like differences in numerical constants or subtle algorithmic complexity differences). They capture semantics in a **statistical** sense rather than a formal one. That said, their ability to generalize across languages and even catch cross-language clones (e.g., the same algorithm in Python vs Java) is a major advantage. Indeed, CodeBERT and GraphCodeBERT are multilingual, allowing detection of similar code across different programming languages, which purely syntax-based approaches would struggle with.

### Experimental Comparison: Semantic vs Lexical/Structural Methods  
Studies that compare these approaches show that **semantic embedding methods tend to outperform lexical and structural methods, especially as clones become more abstract (Type-3, Type-4)**. For instance, on a standard BigCloneBench clone detection, classical tools (like those using AST, tokens, or PDG) reach good recall for Type-1 and Type-2 clones but drop off on Type-3. CodeBERT, by virtue of learned semantics, could detect many more Type-3 clones, giving it a higher overall score. In a qualitative sense, a semantic model might correctly identify that two pieces of code are both doing a DFS traversal of a graph even if one uses recursion and the other uses an explicit stack – something an AST edit distance would see as a big difference. On the other hand, lexical similarity might only catch them if they share specific keywords like `push`/`pop` or the same function names.

However, there are cases where lexical or structural methods still win out or complement the semantic ones:
- If two code fragments are very simple and short (few tokens), TF-IDF or even direct token matching might flag a duplicate faster and more obviously than a large model, which could overthink it. In such cases (like short boilerplate or template code), the semantic model might consider them trivial and not unique, whereas a simple textual diff finds the exact match.
- Semantic models can be confused by **unseen patterns**. If code is out-of-distribution (using an API or language construct the model didn’t see much during training), it might not embed it meaningfully. In contrast, an AST comparison will still work as long as it can parse the code.
- The embeddings are numeric and not transparently interpretable. So, while they often correlate with functional similarity, it can be hard to explain *why* the model thinks two pieces are similar. For sensitive applications (like plagiarism accusations), one might still want the clarity of seeing the common tokens or matched AST subtrees that justify a claim of similarity.

In one experiment evaluating generalizability, researchers found that CodeBERT’s clone detection accuracy dropped when tested on a slightly different set of clones than it was fine-tuned on. This suggests these models may inadvertently overfit to the style of clones in their training data (e.g., they might do great on typical student plagiarism cases, but if given a clever, semantically similar but stylistically very different pair, they might miss it). Meanwhile, approaches that rely on actual execution (like running the code or comparing outputs) would not be fooled by style at all – but those are often impractical to do at scale or may require the code to be runnable in a controlled environment.

In summary, semantic methods (especially transformer-based models) currently lead in benchmarking studies for code similarity, but they are often used in combination with lexical/structural techniques for a balanced solution. Next, we compare all three approaches directly and examine such hybrid strategies.

## Comparative Analysis

To fully understand the trade-offs, we compare lexical, structural, and semantic similarity approaches along several dimensions:

- **Accuracy in detecting meaningful similarity:** Semantic models generally have the highest ceiling here, as they can identify functionally equivalent code that looks different. Structural methods are next, capturing similarities when structure is preserved (renaming, reordering that doesn’t change AST too much). Lexical methods are most limited, basically catching only superficial similarities or exact copies. In a perfect scenario (no tricky transformations), all three can catch clones – for example, two files with minor renaming will be caught by all three. But as the differences become more complex (logic-level similarities with different implementations), lexical fails first, structural might fail if the ASTs diverge, while semantic has the best chance to succeed.

- **False positives and negatives:** Lexical approaches have false negatives on semantically same code with different tokens (e.g., missed Type-4 clones) and false positives on code with similar tokens but different behavior. Structural approaches reduce false negatives for renamed or slightly reordered code (they handle Type-2 well), but still have false negatives when the structure truly changes (e.g., different algorithms). They also can have false positives when structures coincide (as discussed, similar AST for different purpose) because they lack semantic understanding. Semantic approaches reduce those false positives – a good semantic model “realizes” that temperature conversion and currency conversion have different semantics because maybe the identifiers `celsius` vs `usd` have different contextual embeddings, etc. That said, semantic models might occasionally give high similarity to unrelated code if they share an uncommon pattern the model learned (an odd false positive), or miss clones if they use divergent implementations that the model doesn’t connect. Overall, semantic methods strive to align with human intuition of similarity more closely than the others, which mostly look at form. Indeed, combining multiple metrics often shows that semantic embeddings align with execution-similarity or functional tests closer than lexical metrics do ([Source Code Clone Detection Using Unsupervised Similarity Measures](https://arxiv.org/html/2401.09885v1#:~:text=Table%201%20compares%20various%20unsupervised,would%20be%20good)).

- **Performance and scalability:** Lexical methods are winners in speed and simplicity. Comparing TF-IDF vectors with cosine is extremely fast (after the initial vectorization which itself can be optimized with sparse representations) and can scale to millions of documents with indexing. Structural methods are slower – parsing code is extra overhead, and comparing tree structures is costlier. But many structural clone detectors have optimized algorithms or use indexing on subtree hashes to achieve scalable performance (tools like Deckard, NiCad, CCFinder etc., each use clever tricks to manage large inputs). Semantic methods, especially deep learning models, have the overhead of neural network inference. Computing an embedding requires running the code through a transformer, which is quite expensive per input compared to simple tokenization. However, one can embed all code offline and then compare vectors, similar to lexical. The comparison of embedding vectors (using cosine) is fast like any vector similarity, so the main cost is in the embedding computation. With modern hardware and the ability to batch process, even embedding hundreds of thousands of functions is feasible, but it’s still a heavier process than TF-IDF. Additionally, the model’s size (CodeBERT has hundreds of millions of parameters) is a factor – using it requires significant memory and possibly GPU acceleration.

- **Interpretability:** This is a non-numerical but important factor. Lexical similarity is highly interpretable – we can easily explain why two pieces of code were deemed similar (“they share 80% of tokens, see these common keywords, etc.”). Structural similarity is also interpretable – one can point to the AST mapping or edit operations (“you can transform code A to code B by these 3 insertions/deletions, which is a small change”). Semantic embeddings, conversely, operate as a black box. It’s difficult to directly explain what features of the code led to a high similarity score. Recent work is looking into making these models more interpretable, for example by visualizing which tokens in code contributed to the similarity decision. Still, for now, if a CodeBERT embedding says two snippets are 95% similar, one has to trust the model’s training rather than a transparent rationale (this lack of transparency is why some tools combine approaches, using semantic filtering but then showing lexical evidence when reporting a clone).

To illustrate the differences, consider a **case study** with three code snippets:
1. Snippet A: five sequential print statements (the "Welcome to Java" example).
2. Snippet B: the same output produced using a loop.
3. Snippet C: a different message printed five times.

Lexical comparison (bag-of-words) might say A vs B are not very similar (because one has multiple `"Welcome to Java"` tokens and no loop token, the other has one `"Welcome to Java"` and a `for` loop token). It might, however, say A vs C are somewhat similar if both share many tokens like `System.out.println` repeated, even though the content differs slightly. AST structural comparison would show A vs B have a difference in structure (five print statements vs one loop node), giving a moderate difference score ([Source Code Clone Detection Using Unsupervised Similarity Measures](https://arxiv.org/html/2401.09885v1#:~:text=Table%201%20compares%20various%20unsupervised,would%20be%20good)). A vs C structurally are quite similar (both have five print statements in a row), so AST distance would be small – even though semantically C differs if the message is different. A semantic embedding model would likely recognize A and B as doing the same repetition of a message (high similarity), and also notice that A vs C differ in the string literal (somewhat lower similarity depending on how it handles string content). This hypothetical scenario underscores how each method perceives similarity: lexical sees token overlap, structural sees shape, semantic sees purpose (to the extent it can).

### Performance Benchmarks and Studies  
Multiple studies have empirically compared these approaches. Ragkhitwetsagul *et al.* (2018) compared 30 code similarity tools (text-based, AST-based, metrics-based, etc.) on several scenarios. They found that under heavy transformations (“pervasive modifications” that significantly change the code layout), simple textual similarity measures can perform as well as specialized tools – essentially because when code is very transformed, most methods struggle and any surviving clues (like some textual patterns) are equally usable by text-based techniques. However, in cases of boilerplate (where a lot of code is structurally similar but semantically meaningless, like license headers or repeated templates), the more **specialized code similarity techniques outperformed textual measures**, since they could filter out unimportant parts or use structural cues. Their study also validated that heavy normalization (e.g., compiling and decompiling code to a uniform format) dramatically improves clone detection across tools, reducing false differences. This suggests a hybrid pipeline: use normalization and then apply either lexical or structural similarity, which many practical tools do.

On modern benchmarks like CodeXGLUE (which includes clone detection tasks), the top results are dominated by transformer-based (semantic) models. For example, CodeT5 (a T5-based code model) and CodeBERT report clone detection F1 scores in the mid-90s% on BigCloneBench, whereas older AST or token-based methods typically report lower numbers on the same benchmark (often 10-20 points lower in recall for the tougher clones). Another benchmark is semantic function search (finding a function that does X in a library); embeddings again shine there by retrieving correct matches even if the code is written differently.

It’s worth noting that *execution-based metrics* can be a gold standard for similarity: for instance, running two code fragments with various inputs to see if outputs match (or using techniques like symbolic execution to see if their functions are equivalent). Some research uses execution results as a measure (e.g., “Execution match” where two code solutions are considered similar if they produce the same outputs on a test suite). AST-based similarity has been found to correlate well with execution match in some studies, but pure lexical similarity correlates less (since it doesn’t truly measure correctness). Of course, running code for arbitrary snippets is not always feasible, and it only works if you have test cases and the code is runnable, so it’s not a general solution for clone detection in large codebases. But it shows that the ultimate measure of semantic similarity is functional equivalence, which current semantic models approximate but do not guarantee.

### Hybrid Models Combining Multiple Techniques  
Given the complementary strengths of lexical, structural, and semantic methods, recent work has explored **hybrid models** that integrate multiple sources of information. The goal is to preserve heterogeneous features of code (textual, syntactic, data-flow, etc.) and leverage them together for a more robust similarity measure. One example is the FCCA framework (Functional Code Clone Analysis) by Sun *et al.* (2020), which creates a combined representation of code using textual tokens, AST, and control-flow graph (CFG) information. In their approach, they embed each of these representations and then *fuse* them (through an attention mechanism in a neural network) into a single embedding. The attention mechanism learns to weight the importance of lexical vs syntactic vs structural features for the task at hand. This way, the model can, for instance, pay more attention to AST structure when variable names have all been changed, but pay attention to textual details when the structure is mostly the same but perhaps an important literal or operator changed.

Hybrid approaches can also be non-ML: for example, a plagiarism detection tool might first do a quick lexical similarity scan to filter candidate pairs (fast elimination of obviously dissimilar code), then apply an AST edit distance on the short-listed pairs for more precise comparison, and finally maybe verify semantic equivalence by running tests. In fact, industrial tools often combine steps: **textual pre-filtering** + **structural analysis** is common to reduce the computational load and false positives. Another hybrid strategy is **token+PDG**: some research represents code as a combination of token sequences and program dependency graphs, comparing both aspects (if both token similarity and dependency similarity are high, then it's a strong clone candidate).

The advantage of hybrids is clearly demonstrated when detecting **functional clones** (Type-4) that are very hard: one might need to notice that two functions share a lot of rare tokens (lexical clue) *and* have a similar loop/branch structure (structural clue) *and* perhaps use similar variable misuse patterns (semantic clue). Each alone might not be convincing, but together they indicate a likely clone. As one paper puts it, *“a precise clone detector requires a hybrid code representation that preserves heterogeneous code features... Simply relying on an individual feature may result in inaccurate results.”*. By combining features, hybrid models have achieved higher accuracy than single-technique methods in detecting functional clones.

One current line of hybrid research is combining neural embeddings with logic or symbolic reasoning. For instance, a transformer might encode code into a vector, but then an analysis component could check if certain important behaviors (like API call sequences or output traces) match between two snippets. Such combinations could mitigate the weakness of neural models on out-of-distribution code by falling back on symbolic checks.

In summary, no single approach is universally best: lexical methods are easy and catch exact clones, structural methods handle renaming and structural similarity, and semantic methods go deepest to capture logic – but can be opaque or computationally heavy. Hybrid approaches strive to get the best of all worlds, and empirical evidence suggests they can significantly improve clone detection performance when carefully designed.

## Applications and Future Directions

The ability to measure code similarity has many practical applications in software engineering and computer science research. We discuss a few key areas and how the above methods apply, as well as future improvements and trends (such as more advanced hybrid models and normalization techniques).

### Code Plagiarism Detection  
Detecting plagiarism in source code (such as students copying assignments or developers stealing code) is a classic application of code similarity metrics. Many plagiarism detectors (like MOSS, JPlag, etc.) historically use token-based indexing and substring matching (winnowing algorithms) to find large overlaps between files – essentially lexical methods geared for speed and robustness to small edits. For cross-language plagiarism or more advanced copying (where code is restructured), structural and semantic methods provide an edge.

Plagiarism detection typically operates under the assumption that plagiarized code will have *unusual similarity* with some reference or peer submission. Lexical TF-IDF + cosine can be used: one can treat each student’s program as a document and compute similarity with others, flagging pairs that score above a threshold. This catches straightforward copying. To catch smarter obfuscation, tools incorporate normalization: renaming all identifiers, formatting code consistently, removing comments, etc., before comparison. By doing so, Type-2 clones (where a student just renamed variables) are easily caught. One approach even suggests using an *IDF-like weighting* to downplay very common code (like provided template code) and upweight rare snippets, to reduce false alarms from boilerplate that everyone has.

When plagiarism crosses language boundaries (e.g., a student translates a Java solution into C++), lexical comparison fails. Here, abstract representations or semantics help. One research effort applied a TF-IDF style weighting to *normalized tokens that are language-agnostic* (for example, both languages’ loops might be normalized to a generic "FOR" token) and could detect cross-language copies with good accuracy. More recently, using language-neutral embeddings (like encoding both Java and C++ code with the same CodeBERT model) allows comparison of code across languages – if the embeddings are in the same vector space, a copied algorithm in two languages should yield similar vectors.

In practice, plagiarism detectors often combine approaches: do a fast lexical scan to shortlist, then perhaps an AST or semantic similarity check for borderline cases. The tolerance for false positives is low here due to the serious accusation of plagiarism, so interpretable evidence is important (e.g., showing the common subsequences of code). Lexical and structural methods provide that evidence (a listing of common token sequences or an alignment of AST nodes). Semantic similarity can serve as a powerful hidden feature to find the needle in the haystack (e.g., flag two submissions that have no obvious textual overlap but are unusually similar in functionality).

It’s been observed that from a *practical* standpoint, the outcome of plagiarism and clone detection is the same: two pieces of code identified as similar. The difference is intent (plagiarism is intentional copying, clones in a codebase might be for convenience or forked evolution). Therefore, improvements in clone detection techniques directly benefit plagiarism detection. We can expect future plagiarism tools to incorporate AI-based semantic analysis to catch even more sophisticated cheating, possibly even attempting to detect if one code is a trivial transformation of another (something a model might recognize even if the transformation is semantics-preserving).

### Automated Refactoring and Code Review  
Finding similar code segments is useful in **refactoring** – the process of restructuring existing code without changing its behavior. A common refactoring is to eliminate duplicate code by pulling it out into a shared function or module. To do this systematically, tools need to identify duplicate or near-duplicate code fragments. Structural similarity detectors (like clone detectors) are often used in IDEs or analysis tools to highlight clones in a codebase. By knowing where the clones are, a developer or an automated refactoring tool can suggest merging them. For instance, if two large code blocks across a project are 90% identical (perhaps copy-pasted with slight changes), that’s a candidate to refactor into one generic function with parameters for the differences.

Lexical methods can find the obvious copy-pastes (exact or with renaming), while semantic methods might suggest refactoring even when the code isn’t copy-pasted but just *similar in purpose*. An emerging idea is *semantic refactoring assistants* – these would detect when two pieces of code do the same job and suggest consolidating them, even if they were written independently. Transformer models that understand code functionality could enable this. For example, GraphCodeBERT’s authors mention that their similarity approach can help in *“identifying opportunities for refactoring”* by detecting semantically similar fragments and clarifying why they are similar.

In code review, similarity detection can be used to find if new code coming in via a commit or pull request is similar to code elsewhere (perhaps indicating duplicate logic that could be reused or a possible library function that the new code reimplements). It can also catch inconsistencies: if a project has multiple implementations of the same functionality, a reviewer might want to know why. Tools like **Simian** or **PMD's CPD (Copy/Paste Detector)** scan for duplicates to aid code quality. Future code review tools might incorporate semantic clone detection to go beyond copy-paste and catch even logical redundancy.

Another angle in code review is using large language models (like OpenAI Codex) to suggest improvements by comparing the code against learned best practices or common implementations. If a piece of code is semantically similar to a known algorithm, the tool can flag “this looks like bubble sort; consider using the library sort function” or “we have an implementation of this in module X, consider reusing it.” Such suggestions rely on the model recognizing similarity in intent at a higher level and could become more common as AI gets integrated into development environments.

### Software Maintenance and Debugging  
Code similarity is also useful in maintaining software and debugging. **Bug propagation in clones** is a known issue: if you have duplicated code and a bug is found in one instance, chances are the other instances have the same bug. Hence, detecting clones allows maintainers to find all locations that need the bug fix. Conversely, if a bug is fixed in one clone but not another, the two clones become *inconsistent*, which can be a sign of trouble.

There’s an interesting application of semantic similarity in bug detection: find functions that are supposed to be similar (e.g., two implementations of a protocol) and see if one behaves differently. A recent approach named *FICS* (Functionally Similar yet Inconsistent Code Snippets) does exactly this – it looks for clusters of similar code in a codebase and flags inconsistencies among them as potential bugs. The intuition is that if you have multiple implementations of, say, a graph traversal, and one of them lacks a check that all the others have (like a null-pointer check or boundary check), that one is likely buggy. By using a code embedding or other similarity measure to find the cluster of “functionally similar” code, and then doing a differential analysis, such tools can detect subtle bugs without any test cases. This is a powerful paradigm: **using code similarity to find anomalies**. It relies heavily on accurate semantic similarity – the tool must be sure the code snippets are meant to do the same thing. If it falsely clusters dissimilar code, the differences are not bugs but intended. So high precision semantic clone detection is key for this application.

Software maintenance also involves understanding the impact of changes. Suppose you plan to refactor or change a function – finding other code that is similar can tell you if those might be impacted or should be changed similarly. For example, if you improve the efficiency of one sorting routine, search for other sorting routines in the code (semantic search) to update them as well. This is like a semantic grep tool: developers could query “find code like this” to locate other spots that might need the same fix or improvement.

For debugging specifically, if you encounter a tricky bug, it might help to find if similar code exists that doesn’t have the bug – then compare the differences. This again is facilitated by similarity detection: find code that is meant to do the same thing (perhaps an earlier version, or a library implementation) and highlight where the problematic code diverges. Those divergences might pinpoint the bug. This approach has been used manually by developers and is being automated by tools like the one mentioned above (FICS) and others.

### Improvements in Hybrid Models and Normalization (Future Directions)  
The frontier of code similarity research is moving toward more **robust hybrid models** and better normalization or canonicalization of code. Some future directions and ongoing developments include:

- **Cross-representation Learning:** Combining learned embeddings with traditional representations explicitly. For instance, a model that jointly learns from code’s AST and its execution traces. Early work in this direction (like combining static and dynamic analysis for similarity) shows improved resilience to changes. A hybrid semantic similarity might involve running the code (or a lightweight symbolic execution) to get a behavioral signature and pairing that with an embedding comparison.

- **Language-agnostic similarity:** As software becomes polyglot, detecting similar code across languages is important. Future models might use an *intermediate representation* (like some universal IR or even pseudocode in natural language) to compare algorithms beyond syntax. Large language models could play a role by translating code into a common representation (for example, prompting GPT-4: “Describe the following function in plain English” for two functions and then comparing those descriptions).

- **Normalization and Equivalence:** Advanced normalization could include things like sorting commutative operations (so `a+b` vs `b+a` look the same), abstracting control structures (`for` vs `while` that achieve the same effect), or even solving for symbolic equivalence in small code segments. These ideas border on formal methods. In clone detection, there is research on *transformational approaches*: systematically applying meaning-preserving transformations to see if two codes can become identical. While expensive, in limited domains this can catch clones that other methods can’t.

- **Use of LLMs in the loop:** Instead of just using embeddings, using LLMs to directly answer “are these codes doing the same thing?” or even “rewrite code A in the style of code B and see if it matches B exactly” could be explored. The 2024 study with GPT-4 prompting for similarity is a step in this direction. The instability of answers is a challenge, but as these models improve and can perhaps maintain state or do chain-of-thought, they might provide very accurate judgments along with rationales.

- **Interpretable semantic similarity:** There’s an increasing focus on explaining *why* a similarity was determined (especially for AI models). Future hybrid tools might generate a report: e.g., *“Functions A and B are 90% similar. They both perform a file sort: both read a file, parse lines, then sort them. The only difference is A uses quicksort and B uses mergesort.”* Such explanations would combine natural language understanding of code with structural differencing.

- **Benchmarks and evaluations:** As a future direction, the community is establishing more complex benchmarks (like BigCloneBench++ or code search challenges) that include more semantic cases and even human-defined “similarity” judgments. This will drive methods to handle a wider range of clone types. We can expect hybrid and semantic methods to continue to dominate these benchmarks, but with improvements to recall on the hardest cases.

In conclusion, **code similarity assessment has evolved from simple text matching to sophisticated semantic analysis**. Lexical methods (like TF-IDF and cosine) are fast and useful for simple cases but lack deeper understanding. Structural methods (ASTs and tree edit distance) bring in syntactic awareness and handle many superficial modifications, yet they can be misled by semantic differences. Semantic approaches using transformers (CodeBERT, GraphCodeBERT, Codex) provide a powerful way to identify functionally similar code by learning from vast data, showing superior performance on challenging clone detection tasks. Each approach has its strengths and weaknesses, and as we’ve discussed, combining them yields the best results – as evidenced by research on hybrid models that fuse lexical, syntactic, and semantic features. The applications of these techniques are broad: from catching software plagiarism and duplications, to enabling safer refactoring, to detecting bugs and improving code quality. Ongoing research is likely to make code similarity tools more accurate, more language-general, and more explainable, which will greatly assist developers in managing and understanding ever-growing codebases.

**References:**

- Ragkhitwetsagul, C., Krinke, J., & Clark, D. (2018). *A comparison of code similarity analysers*. Empirical Software Engineering, 23(4), 2464–2519.  
- Roy, C.K., & Cordy, J.R. (2008). *NICAD: Accurate detection of near-miss intentional clones using flexible pretty-printing and code normalization*. ICPC 2008.  
- Jiang, L., Misherghi, G., Su, Z., & Glondu, S. (2007). *Deckard: Scalable and accurate tree-based detection of code clones*. ICSE 2007.  
- Wang, W., et al. (2020). *Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree*. (Used in CodeXGLUE clone detection benchmark).  
- Wang, Z., et al. (2020). *CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation*. (For CodeXGLUE results).  
- Luo, L., et al. (2022). *Semantic Clone Detection: A Deep Learning Approach*. (Discusses CodeBERT performance on semantic clones).  
- Song, Y., et al. (2024). *Revisiting Code Similarity Evaluation with AST Edit Distance*. ACL 2024.  
- Nguyen, A., et al. (2023). *FICS: Finding Inconsistent Code Snippets for Bug Detection*. (Functionally-similar inconsistent code detection).  
- Sun, Y., et al. (2020). *FCCA: Hybrid code representation for functional clone detection*. TSE 2021.  
- Guo, D., et al. (2020). *GraphCodeBERT: Pre-training Code Representations with Data Flow*. (Microsoft Research).  

*(Plus additional sources and documentation as cited in-line.)*

